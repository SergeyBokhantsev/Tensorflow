{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IZX3whF6eQ0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, Input, MaxPooling2D, BatchNormalization, Add\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.regularizers import L2\n",
        "from time import time\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [15, 10]\n",
        "\n",
        "# Set the seeds for reproducibility\n",
        "from numpy.random import seed\n",
        "from tensorflow.random import set_seed\n",
        "seed_value = 1234578790\n",
        "seed(seed_value)\n",
        "set_seed(seed_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62nbrRyzp_hH",
        "outputId": "4aa5fee8-591d-4df9-b0ae-4f8aa5638548"
      },
      "outputs": [],
      "source": [
        "env = 'local'\n",
        "\n",
        "if (env == 'colab'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    train_folder = '/content/drive/MyDrive/Colab Notebooks/FaceUpscale/train'\n",
        "    test_folder = '/content/drive/MyDrive/Colab Notebooks/FaceUpscale/test'\n",
        "    cache_folder = '/content/cache'\n",
        "    train_batch_size = 512\n",
        "\n",
        "if (env == 'local'):\n",
        "    train_folder = '../data/train'\n",
        "    test_folder = '../data/test'\n",
        "    cache_folder = '../data/cache'\n",
        "    train_batch_size = 32\n",
        "\n",
        "x_img_size = 32\n",
        "\n",
        "y_img_size = 128\n",
        "y_img_channels = 3\n",
        "\n",
        "\n",
        "if (cache_folder is not None and not os.path.exists(cache_folder)):\n",
        "    os.makedirs(cache_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7rOluPjp_hI"
      },
      "outputs": [],
      "source": [
        "file_cache_write = 0\n",
        "file_cache_read = 0\n",
        "file_source_read = 0\n",
        "\n",
        "train_files_list = []\n",
        "test_files_list = []\n",
        "\n",
        "def reset_cache_counters():\n",
        "    global file_cache_write \n",
        "    global file_cache_read \n",
        "    global file_source_read\n",
        "    file_cache_write = 0\n",
        "    file_cache_read = 0\n",
        "    file_source_read = 0\n",
        "\n",
        "def image_file_iterator(root):\n",
        "    for subdir, dirs, files in os.walk(root):        \n",
        "        for file in files:\n",
        "            if (file.endswith('.png')):\n",
        "                yield os.path.join(subdir, file)\n",
        "\n",
        "        for dir in dirs:\n",
        "            for file in image_file_iterator(dir):\n",
        "                yield os.path.join(subdir, file)\n",
        "\n",
        "def train_files_shuffled_iterator():\n",
        "    global train_files_list\n",
        "    if (len(train_files_list) == 0):\n",
        "        train_files_list = list(image_file_iterator(train_folder))\n",
        "\n",
        "    np.random.shuffle(train_files_list)\n",
        "\n",
        "    for f in train_files_list:\n",
        "        yield f\n",
        "\n",
        "def test_files_iterator():\n",
        "    global test_files_list\n",
        "    if (len(test_files_list) == 0):\n",
        "        test_files_list = list(image_file_iterator(test_folder))\n",
        "\n",
        "    for f in test_files_list:\n",
        "        yield f\n",
        "\n",
        "def scale_and_normalize(img, size):\n",
        "    if (img.shape[0] != size or img.shape[1] != size):\n",
        "        img = cv2.resize(img, (size, size))\n",
        "    return img / 255\n",
        "\n",
        "def make_cache_file_name(id, img_size, prefix):\n",
        "    return prefix + str(hash(id + str(img_size))) + '.bin'\n",
        "\n",
        "def load_cached_array(id, img_size, prefix):\n",
        "    file = os.path.join(cache_folder, make_cache_file_name(id, img_size, prefix))\n",
        "\n",
        "    if (os.path.exists(file)):\n",
        "        global file_cache_read\n",
        "        file_cache_read += 1\n",
        "        return np.fromfile(file)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def save_array_to_cache(arr, id, img_size, prefix):\n",
        "    file = os.path.join(cache_folder, make_cache_file_name(id, img_size, prefix))\n",
        "\n",
        "    if (not os.path.exists(file)):\n",
        "        global file_cache_write\n",
        "        file_cache_write += 1\n",
        "        arr.tofile(file)\n",
        "\n",
        "def load_cached_xy_train(id):\n",
        "    x_cached = load_cached_array(id, x_img_size, '_x')\n",
        "\n",
        "    if (x_cached is None):\n",
        "        return (None, None)\n",
        "    \n",
        "    y_cached = load_cached_array(id, y_img_size, '_y')\n",
        "\n",
        "    if (y_cached is None):\n",
        "        return (None, None)\n",
        "    \n",
        "    return (x_cached.reshape(x_img_size, x_img_size), y_cached.reshape(y_img_size, y_img_size, 3))\n",
        "\n",
        "def save_xy_train_to_cache(id, x_img, y_img):\n",
        "    save_array_to_cache(x_img, id, x_img_size, \"_x\")\n",
        "    save_array_to_cache(y_img, id, y_img_size, \"_y\")\n",
        "\n",
        "def data_iterator(file_iterator, batch_size):\n",
        "    files_pending = True\n",
        "    while(files_pending):\n",
        "        x_batch = []\n",
        "        y_batch = []\n",
        "\n",
        "        for ii in range(batch_size):\n",
        "            fpath = next(file_iterator, None) \n",
        "\n",
        "            if (fpath is not None):\n",
        "                # try to load cached data\n",
        "                x_img, y_img = load_cached_xy_train(fpath)\n",
        "\n",
        "                if (x_img is None or y_img is None):\n",
        "                    # Load source image\n",
        "                    global file_source_read\n",
        "                    file_source_read += 1\n",
        "                    src_img = cv2.imread(fpath)\n",
        "                    src_img = cv2.cvtColor(src_img, cv2.COLOR_BGR2RGB)\n",
        "                    # Make X\n",
        "                    x_img = cv2.cvtColor(src_img, cv2.COLOR_RGB2GRAY)\n",
        "                    x_img = scale_and_normalize(x_img, x_img_size)\n",
        "                    # Make Y\n",
        "                    y_img = scale_and_normalize(src_img, y_img_size)\n",
        "                    # Save to cache\n",
        "                    save_xy_train_to_cache(fpath, x_img, y_img)\n",
        "\n",
        "                x_batch.append(x_img)\n",
        "                y_batch.append(y_img)\n",
        "\n",
        "            else:\n",
        "                files_pending = False\n",
        "                break\n",
        "        \n",
        "        if len(x_batch) > 0:\n",
        "            yield np.array(x_batch), np.array(y_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "XydHB4mcp_hI",
        "outputId": "f457162d-3de1-470c-94ac-3edb4f3f66c9"
      },
      "outputs": [],
      "source": [
        "# Let's see the number of data\n",
        "train_set_len = sum(1 for _ in image_file_iterator(train_folder))\n",
        "test_set_len = sum(1 for _ in image_file_iterator(test_folder))\n",
        "\n",
        "print(train_set_len)\n",
        "print(test_set_len)\n",
        "\n",
        "# Also do a smoke test for datagen \n",
        "# for 1000 test images we expecting 1000 batches (when batch_size = 1)\n",
        "if (sum(1 for _ in data_iterator(test_files_iterator(), batch_size=1)) != test_set_len):\n",
        "    print('datagen failure!')\n",
        "\n",
        "# for 1000 test images we expecting 500 batches (when batch_size = 2)\n",
        "if (sum(1 for _ in data_iterator(test_files_iterator(), 2)) != test_set_len / 2):\n",
        "    print('datagen failure!')\n",
        "\n",
        "print('Source reads:', file_source_read)\n",
        "print('Cache reads:', file_cache_read)\n",
        "print('Cache writes:', file_cache_write)\n",
        "\n",
        "# Let's visualize cached image to ensure it cached and read OK\n",
        "reset_cache_counters()\n",
        "cached_x_batch, cached_y_batch = next(data_iterator(test_files_iterator(), batch_size=1))\n",
        "# ensure images were load from cache\n",
        "if (file_cache_read == 2):\n",
        "    plt.subplot(121), plt.imshow(cached_x_batch[0], cmap='gray')  \n",
        "    plt.subplot(122), plt.imshow(cached_y_batch[0]) \n",
        "    print('cached x shape:', cached_x_batch[0].shape)\n",
        "    print('cached y shape:', cached_y_batch[0].shape)\n",
        "else:\n",
        "    print('cache failure!')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "mQuLFpLKp_hI",
        "outputId": "bdd51ae3-3ae3-4a9e-e5a6-fa07212091b5"
      },
      "outputs": [],
      "source": [
        "# test datagen\n",
        "batch = next(data_iterator(train_files_shuffled_iterator(), 10))\n",
        "\n",
        "x_train_batch = batch[0]\n",
        "y_train_batch = batch[1]\n",
        "\n",
        "# Show x_train\n",
        "for ii in range(x_train_batch.shape[0]):\n",
        "    plt.subplot(3,5,ii+1), plt.imshow(x_train_batch[ii], cmap = 'gray'), plt.title(ii)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "zMUhFfZXp_hJ",
        "outputId": "12cc42c4-dec6-4288-fcf9-1f5baa403114"
      },
      "outputs": [],
      "source": [
        "# Show y_train\n",
        "for ii in range(y_train_batch.shape[0]):\n",
        "    plt.subplot(3,5,ii+1), plt.imshow(y_train_batch[ii]), plt.title(ii)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_iPVDd8p_hJ"
      },
      "outputs": [],
      "source": [
        "def rdb_block(inputs, layers_count):\n",
        "    # get number of input channels\n",
        "    channels = inputs.get_shape()[-1]\n",
        "    # initialize outputs list\n",
        "    outputs = [inputs]\n",
        "    \n",
        "    # common Conv2D args\n",
        "    conv_args = {\n",
        "        \"activation\": \"relu\",\n",
        "        \"kernel_initializer\": \"Orthogonal\",\n",
        "        \"padding\": \"same\",\n",
        "    }\n",
        "\n",
        "    # Make Residual Dense Block\n",
        "    for _ in range(layers_count):\n",
        "        concatenation = tf.concat(outputs, axis=-1)\n",
        "        net = Conv2D(channels, 3, **conv_args)(concatenation)\n",
        "        outputs.append(net)\n",
        "\n",
        "    # Make final resulting net\n",
        "    final_concatenation = tf.concat(outputs, axis=-1)\n",
        "    final_net = Conv2D(channels, 1, **conv_args)(final_concatenation)\n",
        "\n",
        "    # Add input net and final output net (RDB)\n",
        "    final_net = Add()([final_net, inputs])\n",
        "\n",
        "    return final_net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jhr1lplCp_hJ"
      },
      "outputs": [],
      "source": [
        "def psnr(orig, pred):\n",
        "\t# cast the target images to integer\n",
        "\torig = orig * 255.0\n",
        "\torig = tf.cast(orig, tf.uint8)\n",
        "\torig = tf.clip_by_value(orig, 0, 255)\n",
        "\t# cast the predicted images to integer\n",
        "\tpred = pred * 255.0\n",
        "\tpred = tf.cast(pred, tf.uint8)\n",
        "\tpred = tf.clip_by_value(pred, 0, 255)\n",
        "\t# return the psnr\n",
        "\treturn tf.image.psnr(orig, pred, max_val=255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eGtiMJWp_hJ",
        "outputId": "9fa475fe-5b1e-45e7-fb17-c2f5ba6b0c1b"
      },
      "outputs": [],
      "source": [
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "\n",
        "conv_args = {\n",
        "        \"activation\": \"relu\",\n",
        "        \"kernel_initializer\": \"Orthogonal\",\n",
        "        \"padding\": \"same\",\n",
        "    }\n",
        "\n",
        "scale_ratio = y_img_size / x_img_size\n",
        "print('Scale ratio: ', scale_ratio)\n",
        "\n",
        "inputs = Input(shape=(x_img_size, x_img_size, 1))\n",
        "net = Conv2D(64, 5, **conv_args)(inputs)\n",
        "net = Conv2D(64, 3, **conv_args)(net)\n",
        "# Adding RDB Block\n",
        "net = rdb_block(net, layers_count=7)\n",
        "net = Conv2D(32, 3, **conv_args)(net)\n",
        "# Another one RDB Block\n",
        "net = rdb_block(net, layers_count=7)\n",
        "# Pixel Shuffle magic here\n",
        "net = Conv2D(y_img_channels * (scale_ratio ** 2), 3, **conv_args)(net)\n",
        "outputs = tf.nn.depth_to_space(net, scale_ratio)\n",
        "\n",
        "#net = Conv2D(128, 13, **conv_args)(inputs)\n",
        "#net = MaxPooling2D(2)(net)\n",
        "#net = Conv2D(64, 5, **conv_args)(inputs)\n",
        "#net = Conv2D(64, 3, **conv_args)(net)\n",
        "#net = Dropout(0.1)(net)\n",
        "#net = Conv2D(32, 3, **conv_args)(net)\n",
        "#net = Conv2D(3 * (scale_ratio ** 2), 3, **conv_args)(net)\n",
        "#net = Dropout(0.2)(net)\n",
        "#outputs = tf.nn.depth_to_space(net, scale_ratio)\n",
        "\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLPjvz5D8exs"
      },
      "outputs": [],
      "source": [
        "def datagen(batch_size):\n",
        "    while(True):\n",
        "        iterator = data_iterator(train_files_shuffled_iterator(), batch_size)\n",
        "        result = next(iterator, None)\n",
        "\n",
        "        if (result is None):\n",
        "            iterator = data_iterator(train_files_shuffled_iterator(), batch_size)\n",
        "        else:\n",
        "            yield result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "tHoOeq9op_hK",
        "outputId": "1466c0c3-ec7d-4c69-feda-1344a015bff9"
      },
      "outputs": [],
      "source": [
        "# Train the network\n",
        "epochs = 10\n",
        "steps_per_epoch = train_set_len / train_batch_size + 1\n",
        "\n",
        "print(steps_per_epoch)\n",
        "\n",
        "x_test, y_test = next(data_iterator(test_files_iterator(), test_set_len))\n",
        "\n",
        "print(len(x_test))\n",
        "\n",
        "#validation_data = (x_test), np.array(y_test))\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=psnr)\n",
        "#history = model.fit(src, steps_per_epoch=steps_per_epoch, epochs=epochs, validation_data=validation_data)\n",
        "history = model.fit(datagen(train_batch_size), steps_per_epoch=steps_per_epoch, epochs=epochs, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1R0oKVRp_hK"
      },
      "outputs": [],
      "source": [
        "test_range = 20\n",
        "\n",
        "batch = next(datagen(test_folder, test_range))\n",
        "\n",
        "x_test = batch[0]\n",
        "\n",
        "y_test = model.predict(x_test)\n",
        "\n",
        "#img = np.array(y_test[9])\n",
        "\n",
        "#plt.imshow(img)\n",
        "\n",
        "#for ii in range(0, test_range-1):\n",
        "#    plt.subplot(10, 2, 1), plt.imshow(x_test[ii], cmap='gray')\n",
        "#    plt.subplot(10, 2, 2), plt.imshow(np.array(y_test[ii]))\n",
        "\n",
        "for ii in range(test_range):\n",
        "    f, axarr = plt.subplots(1,3)\n",
        "    axarr[0].imshow(x_test[ii], cmap='gray')\n",
        "    axarr[1].imshow(cv2.resize(x_test[ii], (y_img_size, y_img_size), interpolation=cv2.INTER_LINEAR), cmap='gray')\n",
        "    axarr[2].imshow(np.array(y_test[ii]))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 ('cv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "b6c733efa17d1e9f6237ee30a57df6101a3b9e76b9636c4b405ae372ec99fb54"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
