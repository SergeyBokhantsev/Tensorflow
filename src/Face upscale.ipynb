{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IZX3whF6eQ0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, Input, MaxPooling2D, BatchNormalization, Add\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.regularizers import L2\n",
        "from time import time\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [15, 10]\n",
        "\n",
        "# Set the seeds for reproducibility\n",
        "from numpy.random import seed\n",
        "from tensorflow.random import set_seed\n",
        "seed_value = 1234578790\n",
        "seed(seed_value)\n",
        "set_seed(seed_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62nbrRyzp_hH",
        "outputId": "5adcbb50-f31e-41ab-8ab6-f8b268f4ecb7"
      },
      "outputs": [],
      "source": [
        "env = 'local'\n",
        "\n",
        "if (env == 'colab'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    #cp -r /content/drive/MyDrive/ColabNotebooks/FaceUpscale/data.zip /content\n",
        "    #7z x data.zip\n",
        "\n",
        "    #unzip data.zip /content/data\n",
        "\n",
        "    #unzip /content/drive/MyDrive/ColabNotebooks/FaceUpscale/data.zip /content/data\n",
        "    #7z x /content/drive/MyDrive/ColabNotebooks/FaceUpscale/data.zip /content/data -o: /content/data\n",
        "\n",
        "    train_folder = '/content/train'\n",
        "    test_folder = '/content/test'\n",
        "    cache_folder = '/content/cache'\n",
        "    train_batch_size = 32\n",
        "\n",
        "if (env == 'local'):\n",
        "    train_folder = '../data/train'\n",
        "    test_folder = '../data/test'\n",
        "    cache_folder = '../data/cache'\n",
        "    train_batch_size = 64\n",
        "\n",
        "file_cache_enabled = False\n",
        "\n",
        "x_img_size = 32\n",
        "\n",
        "y_img_size = 128\n",
        "y_img_channels = 3\n",
        "\n",
        "\n",
        "if (file_cache_enabled and cache_folder is not None and not os.path.exists(cache_folder)):\n",
        "    os.makedirs(cache_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7rOluPjp_hI"
      },
      "outputs": [],
      "source": [
        "file_cache_write = 0\n",
        "file_cache_read = 0\n",
        "file_source_read = 0\n",
        "\n",
        "train_files_list = []\n",
        "test_files_list = []\n",
        "\n",
        "def reset_cache_counters():\n",
        "    global file_cache_write \n",
        "    global file_cache_read \n",
        "    global file_source_read\n",
        "    file_cache_write = 0\n",
        "    file_cache_read = 0\n",
        "    file_source_read = 0\n",
        "\n",
        "def image_file_iterator(root):\n",
        "    for subdir, dirs, files in os.walk(root):        \n",
        "        for file in files:\n",
        "            if (file.endswith('.png')):\n",
        "                yield os.path.join(subdir, file)\n",
        "\n",
        "        for dir in dirs:\n",
        "            for file in image_file_iterator(dir):\n",
        "                yield os.path.join(subdir, file)\n",
        "\n",
        "def train_files_shuffled_iterator():\n",
        "    global train_files_list\n",
        "    if (len(train_files_list) == 0):\n",
        "        train_files_list = list(image_file_iterator(train_folder))\n",
        "\n",
        "    np.random.shuffle(train_files_list)\n",
        "\n",
        "    for f in train_files_list:\n",
        "        yield f\n",
        "\n",
        "def test_files_iterator():\n",
        "    global test_files_list\n",
        "    if (len(test_files_list) == 0):\n",
        "        test_files_list = list(image_file_iterator(test_folder))\n",
        "\n",
        "    for f in test_files_list:\n",
        "        yield f\n",
        "\n",
        "def scale_and_normalize(img, size):\n",
        "    if (img.shape[0] != size or img.shape[1] != size):\n",
        "        img = cv2.resize(img, (size, size))\n",
        "    return img / 255\n",
        "\n",
        "def make_cache_file_name(id, img_size, prefix):\n",
        "    return prefix + str(hash(id + str(img_size))) + '.bin'\n",
        "\n",
        "def load_cached_array(id, img_size, prefix):\n",
        "    file = os.path.join(cache_folder, make_cache_file_name(id, img_size, prefix))\n",
        "\n",
        "    if (os.path.exists(file)):\n",
        "        global file_cache_read\n",
        "        file_cache_read += 1\n",
        "        return np.fromfile(file)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def save_array_to_cache(arr, id, img_size, prefix):\n",
        "    file = os.path.join(cache_folder, make_cache_file_name(id, img_size, prefix))\n",
        "\n",
        "    if (not os.path.exists(file)):\n",
        "        global file_cache_write\n",
        "        file_cache_write += 1\n",
        "        arr.tofile(file)\n",
        "\n",
        "def load_cached_xy_train(id):\n",
        "    x_cached = load_cached_array(id, x_img_size, '_x')\n",
        "\n",
        "    if (x_cached is None):\n",
        "        return (None, None)\n",
        "    \n",
        "    y_cached = load_cached_array(id, y_img_size, '_y')\n",
        "\n",
        "    if (y_cached is None):\n",
        "        return (None, None)\n",
        "    \n",
        "    return (x_cached.reshape(x_img_size, x_img_size), y_cached.reshape(y_img_size, y_img_size, 3))\n",
        "\n",
        "def save_xy_train_to_cache(id, x_img, y_img):\n",
        "    save_array_to_cache(x_img, id, x_img_size, \"_x\")\n",
        "    save_array_to_cache(y_img, id, y_img_size, \"_y\")\n",
        "\n",
        "def data_iterator(file_iterator, batch_size):\n",
        "    global file_cache_enabled\n",
        "    files_pending = True\n",
        "    while(files_pending):\n",
        "        x_batch = []\n",
        "        y_batch = []\n",
        "\n",
        "        for ii in range(batch_size):\n",
        "            fpath = next(file_iterator, None) \n",
        "\n",
        "            if (fpath is not None):\n",
        "\n",
        "                x_img = None\n",
        "                y_img = None\n",
        "\n",
        "                if (file_cache_enabled):\n",
        "                  # try to load cached data\n",
        "                  x_img, y_img = load_cached_xy_train(fpath)\n",
        "\n",
        "                if (x_img is None or y_img is None):\n",
        "                    # Load source image\n",
        "                    global file_source_read\n",
        "                    file_source_read += 1\n",
        "                    src_img = cv2.imread(fpath)\n",
        "                    src_img = cv2.cvtColor(src_img, cv2.COLOR_BGR2RGB)\n",
        "                    # Make X\n",
        "                    x_img = cv2.cvtColor(src_img, cv2.COLOR_RGB2GRAY)\n",
        "                    x_img = scale_and_normalize(x_img, x_img_size)\n",
        "                    # Make Y\n",
        "                    y_img = scale_and_normalize(src_img, y_img_size)\n",
        "\n",
        "                    if (file_cache_enabled):\n",
        "                      # Save to cache\n",
        "                      save_xy_train_to_cache(fpath, x_img, y_img)\n",
        "\n",
        "                x_batch.append(x_img)\n",
        "                y_batch.append(y_img)\n",
        "\n",
        "            else:\n",
        "                files_pending = False\n",
        "                break\n",
        "        \n",
        "        if len(x_batch) > 0:\n",
        "            yield np.array(x_batch), np.array(y_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XydHB4mcp_hI",
        "outputId": "e2907d44-290f-42f1-cc18-5208d3ddca9c"
      },
      "outputs": [],
      "source": [
        "# Let's see the number of data\n",
        "train_set_len = sum(1 for _ in image_file_iterator(train_folder))\n",
        "test_set_len = sum(1 for _ in image_file_iterator(test_folder))\n",
        "\n",
        "print(train_set_len)\n",
        "print(test_set_len)\n",
        "\n",
        "# Also do a smoke test for datagen \n",
        "# for 1000 test images we expecting 1000 batches (when batch_size = 1)\n",
        "if (sum(1 for _ in data_iterator(test_files_iterator(), batch_size=1)) != test_set_len):\n",
        "    print('datagen failure!')\n",
        "\n",
        "# for 1000 test images we expecting 500 batches (when batch_size = 2)\n",
        "if (sum(1 for _ in data_iterator(test_files_iterator(), 2)) != test_set_len / 2):\n",
        "    print('datagen failure!')\n",
        "\n",
        "print('Source reads:', file_source_read)\n",
        "print('Cache reads:', file_cache_read)\n",
        "print('Cache writes:', file_cache_write)\n",
        "\n",
        "# Let's visualize cached image to ensure it cached and read OK\n",
        "reset_cache_counters()\n",
        "cached_x_batch, cached_y_batch = next(data_iterator(test_files_iterator(), batch_size=1))\n",
        "# ensure images were load from cache\n",
        "if (file_cache_read == 2):\n",
        "    plt.subplot(121), plt.imshow(cached_x_batch[0], cmap='gray')  \n",
        "    plt.subplot(122), plt.imshow(cached_y_batch[0]) \n",
        "    print('cached x shape:', cached_x_batch[0].shape)\n",
        "    print('cached y shape:', cached_y_batch[0].shape)\n",
        "else:\n",
        "    print('cache failure!')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "mQuLFpLKp_hI",
        "outputId": "239de7a2-1132-4a2a-9cb7-f21ac1197a02"
      },
      "outputs": [],
      "source": [
        "# test datagen\n",
        "batch = next(data_iterator(train_files_shuffled_iterator(), 10))\n",
        "\n",
        "x_train_batch = batch[0]\n",
        "y_train_batch = batch[1]\n",
        "\n",
        "# Show x_train\n",
        "for ii in range(x_train_batch.shape[0]):\n",
        "    plt.subplot(3,5,ii+1), plt.imshow(x_train_batch[ii], cmap = 'gray'), plt.title(ii)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "zMUhFfZXp_hJ",
        "outputId": "859df10a-1bdd-4f6a-f726-456b65e6bc7c"
      },
      "outputs": [],
      "source": [
        "# Show y_train\n",
        "for ii in range(y_train_batch.shape[0]):\n",
        "    plt.subplot(3,5,ii+1), plt.imshow(y_train_batch[ii]), plt.title(ii)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub_-KNa-s7v5"
      },
      "outputs": [],
      "source": [
        "# Data Augmentation\n",
        "\n",
        "def rotate_image(image, angle):\n",
        "  image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
        "  rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
        "  result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
        "  return result\n",
        "\n",
        "def make_augmentation(input_batch):\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    input_batch_len = input_batch[0].shape[0]\n",
        "\n",
        "    sharp_k = 1.2\n",
        "    sharp_kernel = np.array([[-sharp_k,-sharp_k,-sharp_k], [-sharp_k,9*sharp_k,-sharp_k], [-sharp_k,-sharp_k,-sharp_k]])\n",
        "\n",
        "    for ii in range(input_batch_len):\n",
        "        input_x_img = input_batch[0][ii]\n",
        "        input_y_img = input_batch[1][ii] \n",
        "\n",
        "        # Sharpened\n",
        "        x_list.append(cv2.filter2D(input_x_img, -1, sharp_kernel))\n",
        "        y_list.append(cv2.filter2D(input_y_img, -1, sharp_kernel))\n",
        "\n",
        "        # Rotate CW\n",
        "        x_list.append(rotate_image(input_x_img, -15))\n",
        "        y_list.append(rotate_image(input_y_img, -15))\n",
        "\n",
        "        # Rotate CCW\n",
        "        x_list.append(rotate_image(input_x_img, 15))\n",
        "        y_list.append(rotate_image(input_y_img, 15))\n",
        "\n",
        "        # Flip horizontaly\n",
        "        x_flipped = cv2.flip(input_x_img, 1)\n",
        "        y_flipped = cv2.flip(input_y_img, 1)\n",
        "        x_list.append(x_flipped)\n",
        "        y_list.append(y_flipped)\n",
        "\n",
        "        # Sharpened Flipped\n",
        "        x_list.append(cv2.filter2D(x_flipped, -1, sharp_kernel))\n",
        "        y_list.append(cv2.filter2D(y_flipped, -1, sharp_kernel))\n",
        "\n",
        "        # Rotate CW flipped\n",
        "        x_list.append(rotate_image(x_flipped, -15))\n",
        "        y_list.append(rotate_image(y_flipped, -15))\n",
        "\n",
        "        # Rotate CCW flipped\n",
        "        x_list.append(rotate_image(x_flipped, 15))\n",
        "        y_list.append(rotate_image(y_flipped, 15))\n",
        "\n",
        "    return (np.array(x_list), np.array(y_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_iPVDd8p_hJ"
      },
      "outputs": [],
      "source": [
        "def rdb_block(inputs, layers_count):\n",
        "    # get number of input channels\n",
        "    channels = inputs.get_shape()[-1]\n",
        "    # initialize outputs list\n",
        "    outputs = [inputs]\n",
        "    \n",
        "    # common Conv2D args\n",
        "    conv_args = {\n",
        "        \"activation\": \"relu\",\n",
        "        \"kernel_initializer\": \"Orthogonal\",\n",
        "        \"padding\": \"same\",\n",
        "    }\n",
        "\n",
        "    # Make Residual Dense Block\n",
        "    for _ in range(layers_count):\n",
        "        concatenation = tf.concat(outputs, axis=-1)\n",
        "        net = Conv2D(channels, 3, **conv_args)(concatenation)\n",
        "        outputs.append(net)\n",
        "\n",
        "    # Make final resulting net\n",
        "    final_concatenation = tf.concat(outputs, axis=-1)\n",
        "    final_net = Conv2D(channels, 1, **conv_args)(final_concatenation)\n",
        "\n",
        "    # Add input net and final output net (RDB)\n",
        "    final_net = Add()([final_net, inputs])\n",
        "\n",
        "    return final_net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jhr1lplCp_hJ"
      },
      "outputs": [],
      "source": [
        "def psnr(orig, pred):\n",
        "\t# crop the image (take image center part)\n",
        "\tmargin = int(y_img_size / 4)\n",
        "\torig = orig[margin:y_img_size-margin, margin:y_img_size-margin]\n",
        "\tpred = pred[margin:y_img_size-margin, margin:y_img_size-margin]\n",
        "\t# cast the target images to integer\n",
        "\torig = orig * 255.0\n",
        "\torig = tf.cast(orig, tf.uint8)\n",
        "\torig = tf.clip_by_value(orig, 0, 255)\n",
        "\t# cast the predicted images to integer\n",
        "\tpred = pred * 255.0\n",
        "\tpred = tf.cast(pred, tf.uint8)\n",
        "\tpred = tf.clip_by_value(pred, 0, 255)\n",
        "\t# return the psnr\n",
        "\treturn tf.image.psnr(orig, pred, max_val=255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eGtiMJWp_hJ",
        "outputId": "309871b8-d50c-4e6a-ab5e-1b39f3d3159d"
      },
      "outputs": [],
      "source": [
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "\n",
        "conv_args = {\n",
        "        \"activation\": \"relu\",\n",
        "        \"kernel_initializer\": \"Orthogonal\",\n",
        "        \"padding\": \"same\",\n",
        "    }\n",
        "\n",
        "scale_ratio = y_img_size / x_img_size\n",
        "print('Scale ratio: ', scale_ratio)\n",
        "\n",
        "inputs = Input(shape=(x_img_size, x_img_size, 1))\n",
        "net = Conv2D(64, 5, **conv_args)(inputs)\n",
        "net = Conv2D(64, 3, **conv_args)(net)\n",
        "# Adding RDB Block\n",
        "net = rdb_block(net, layers_count=8)\n",
        "net = Conv2D(32, 3, **conv_args)(net)\n",
        "# Another one RDB Block\n",
        "net = rdb_block(net, layers_count=8)\n",
        "# Pixel Shuffle magic here\n",
        "net = Conv2D(y_img_channels * (scale_ratio ** 2), 3, **conv_args)(net)\n",
        "outputs = tf.nn.depth_to_space(net, scale_ratio)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLPjvz5D8exs"
      },
      "outputs": [],
      "source": [
        "def datagen(batch_size):\n",
        "    iterator = data_iterator(train_files_shuffled_iterator(), batch_size)\n",
        "    while(True):\n",
        "        result = next(iterator, None)\n",
        "\n",
        "        if (result is None):\n",
        "            iterator = data_iterator(train_files_shuffled_iterator(), batch_size)\n",
        "        else:\n",
        "            yield result\n",
        "            yield make_augmentation(result)\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "jcvpiRFBs7v9",
        "outputId": "cf098375-4eb5-4537-ac5f-71015742d22e"
      },
      "outputs": [],
      "source": [
        "# Test datagen\n",
        "def do_test_datagen():\n",
        "    dg = datagen(1)\n",
        "    img_n = 0\n",
        "\n",
        "    # do request datagen for two times:\n",
        "    # every first yields original x and y\n",
        "    # every second yields augmentated set of x and y\n",
        "    for _ in range(2):\n",
        "        x_batch, y_batch = next(dg)\n",
        "\n",
        "        for ii in range(x_batch.shape[0]):\n",
        "            img_n += 1\n",
        "            plt.subplot(5,6,img_n), plt.imshow(x_batch[ii], cmap = 'gray'), plt.title('X')\n",
        "            img_n += 1\n",
        "            plt.subplot(5,6,img_n), plt.imshow(y_batch[ii], cmap = 'gray'), plt.title('Y')\n",
        "           \n",
        "do_test_datagen()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHoOeq9op_hK",
        "outputId": "3cbb5215-c4f6-47bf-9f88-4eb60e2809dc"
      },
      "outputs": [],
      "source": [
        "# Train the network\n",
        "epochs = 10\n",
        "steps_per_epoch = train_set_len / train_batch_size + 1\n",
        "\n",
        "print(steps_per_epoch)\n",
        "\n",
        "x_test, y_test = next(data_iterator(test_files_iterator(), test_set_len))\n",
        "\n",
        "print(len(x_test))\n",
        "\n",
        "#validation_data = (x_test), np.array(y_test))\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=psnr)\n",
        "\n",
        "history = model.fit(datagen(train_batch_size), steps_per_epoch=steps_per_epoch, epochs=epochs, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c1R0oKVRp_hK",
        "outputId": "112c7f62-ade4-486f-bda0-23ffc90db9ff"
      },
      "outputs": [],
      "source": [
        "test_range = 20\n",
        "\n",
        "di = data_iterator(test_files_iterator(), test_range)\n",
        "\n",
        "batch = next(di)\n",
        "batch = next(di)\n",
        "\n",
        "x_test = batch[0]\n",
        "\n",
        "y_test = model.predict(x_test)\n",
        "\n",
        "#img = np.array(y_test[9])\n",
        "\n",
        "#plt.imshow(img)\n",
        "\n",
        "#for ii in range(0, test_range-1):\n",
        "#    plt.subplot(10, 2, 1), plt.imshow(x_test[ii], cmap='gray')\n",
        "#    plt.subplot(10, 2, 2), plt.imshow(np.array(y_test[ii]))\n",
        "\n",
        "for ii in range(test_range):\n",
        "    f, axarr = plt.subplots(1,2)\n",
        "    axarr[0].imshow(x_test[ii], cmap='gray')\n",
        "    #axarr[1].imshow(cv2.resize(x_test[ii], (y_img_size, y_img_size), interpolation=cv2.INTER_LINEAR), cmap='gray')\n",
        "    axarr[1].imshow(np.array(y_test[ii]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaMK4vcAaWy2",
        "outputId": "bdb89b07-f86a-40d6-f5d5-4490c42de4fc"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/ColabNotebooks/FaceUpscale/model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "VWB6-ZIlbilV",
        "outputId": "43f4590f-4595-49c2-d9d5-ea510be4146d"
      },
      "outputs": [],
      "source": [
        "smile = cv2.imread('/content/drive/MyDrive/ColabNotebooks/FaceUpscale/mc.png')\n",
        "smile = cv2.cvtColor(smile, cv2.COLOR_BGR2GRAY)\n",
        "smile = smile / 255\n",
        "\n",
        "l = []\n",
        "l.append(smile)\n",
        "\n",
        "smile2 = model.predict(np.array(l))\n",
        "\n",
        "plt.imshow(np.array(smile2[0]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 ('cv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "b6c733efa17d1e9f6237ee30a57df6101a3b9e76b9636c4b405ae372ec99fb54"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
